# TensorFlow Model Optimization with TensorRT Workshop ðŸš€

Welcome to the "Optimization and Deployment of TensorFlow Models with TensorRT" workshop! In this interactive workshop, you'll delve into using TensorFlow's integration for TensorRT (TF-TRT) to enhance the inference performance of deep learning models.

## Workshop Objectives

By completing this workshop, you will:

- Optimize various deep learning models using TF-TRT
- Understand how TF-TRT enhances model optimization
- Explore optimization at different precisions: FP32, FP16, and INT8
- Experiment with calibration for INT8 precision optimization
- Investigate the impact of conversion parameters on optimization

## Prerequisites

To make the most of this workshop, it's recommended that you:

- Have proficiency in the Python programming language
- Possess familiarity with deep learning concepts, particularly inference
- Understand TensorFlow and its Keras API

## Workshop Contents

This workshop is organized into several JupyterLab Notebooks:

- **01-intro.ipynb**: You are here! Introduction to the workshop.
- **02-jupyter.ipynb**: (optional) A quick guide on using JupyterLab.
- **03-naive-inference.ipynb**: Review of inference in TensorFlow 2 and introduction to helper functions.
- **04-optimizing-tf-models.ipynb**: Learn how TF-TRT optimizes models for faster inference.
- **05-FP32-conversion.ipynb**: Optimize models using TF-TRT at FP32 precision.
- **06-exercise-FP16-conversion.ipynb**: Hands-on exercise for FP16 precision optimization.
- **07-INT8-inference.ipynb**: Explore TF-TRT's INT8 precision optimization.
- **08-exercise-INT8-conversion.ipynb**: Interactive exercise on data calibration and INT8 precision optimization.
- **09_exercise_min_seg_size_benchmarks.ipynb**: Experiment with minimum segment size conversion parameter and optimize additional models.

Let's optimize your understanding of TensorFlow model optimization with TensorRT! ðŸ“ŠðŸ”¬